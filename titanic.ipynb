{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "data_train = pd.read_csv('Data/train.csv')\n",
    "data_test = pd.read_csv('Data/test.csv')\n",
    "\n",
    "data_train.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "\n",
    "#Embarked\n",
    "sns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pclass\n",
    "sns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data_train,\n",
    "              palette={\"male\": \"blue\", \"female\": \"pink\"},\n",
    "              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming Data and Cleanup \n",
    "\n",
    "# Logical human age groups\n",
    "def simplify_ages(df):\n",
    "    df.Age = df.Age.fillna(-0.5)\n",
    "    bins = (-1, 0, 5, 12, 18, 35, 60, 120)\n",
    "    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Young Adult', 'Adult', 'Senior']\n",
    "    categories = pd.cut(df.Age, bins, labels=group_names)\n",
    "    df.Age = categories\n",
    "    return df\n",
    "\n",
    "# First letter in cabin is more important than the number after\n",
    "def simplify_cabins(df):\n",
    "    df.Cabin = df.Cabin.fillna('N')\n",
    "    df.Cabin = df.Cabin.apply(lambda x: x[0])\n",
    "    return df\n",
    "\n",
    "# Fare organized in quartiles\n",
    "def simplify_fares(df):\n",
    "    df.Fare = df.Fare.fillna(-0.5)\n",
    "    bins = (-1, 0, 8, 15, 31, 1000)\n",
    "    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n",
    "    categories = pd.cut(df.Fare, bins, labels=group_names)\n",
    "    df.Fare = categories\n",
    "    return df\n",
    "\n",
    "# Name and name prefix that indicates the sex\n",
    "# maybe i can drop this\n",
    "def format_name(df):\n",
    "    df['Lname'] = df.Name.apply(lambda x: x.split(' ')[0])\n",
    "    df['NamePrefix'] = df.Name.apply(lambda x: x.split(' ')[1])\n",
    "    return df \n",
    "\n",
    "# Drop columns or features that doesn't matter\n",
    "def drop_features(df):\n",
    "    return df.drop(['Ticket', 'Name'], axis=1)\n",
    "\n",
    "# maybe i can drop this\n",
    "def embarked_nan(df):\n",
    "    df.Embarked = df.Embarked.fillna('U')\n",
    "    return df\n",
    "\n",
    "def transform_features(df):\n",
    "    df = simplify_ages(df)\n",
    "    df = simplify_cabins(df)\n",
    "    df = simplify_fares(df)\n",
    "    df = format_name(df)\n",
    "    df = drop_features(df)\n",
    "    df = embarked_nan(df)\n",
    "    return df\n",
    "\n",
    "data_train = transform_features(data_train)\n",
    "data_test = transform_features(data_test)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transformed data\n",
    "\n",
    "# Age\n",
    "sns.barplot(x=\"Age\", y=\"Survived\", hue=\"Sex\", data=data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cabin\n",
    "sns.barplot(x=\"Cabin\", y=\"Survived\", hue=\"Sex\", data=data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fare\n",
    "sns.barplot(x=\"Fare\", y=\"Survived\", hue=\"Sex\", data=data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert unique labels in columns to numbers\n",
    "# maybe i can drop Emabarked, NamePrefix and Lname\n",
    "from sklearn import preprocessing\n",
    "def encode_features(df_train, df_test):\n",
    "    features = ['Fare', 'Cabin', 'Age', 'Sex', 'Lname', 'NamePrefix', 'Embarked']\n",
    "    df_combined = pd.concat([df_train[features], df_test[features]])\n",
    "    \n",
    "    for feature in features:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le = le.fit(df_combined[feature])\n",
    "        df_train[feature] = le.transform(df_train[feature])\n",
    "        df_test[feature] = le.transform(df_test[feature])\n",
    "    return df_train, df_test\n",
    "    \n",
    "data_train, data_test = encode_features(data_train, data_test)\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple way to resolve the problem\n",
    "# no cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train = data_train.drop(['Survived', 'PassengerId'], axis=1)\n",
    "Y_train = data_train[\"Survived\"]\n",
    "X_test  = data_test.drop(\"PassengerId\", axis=1).copy()\n",
    "X_train.shape, Y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "Y_pred = logreg.predict(X_test)\n",
    "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_df = pd.DataFrame(data_test.columns.delete(0))\n",
    "coeff_df.columns = ['Feature']\n",
    "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n",
    "\n",
    "coeff_df.sort_values(by='Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "svc = SVC(gamma='auto')\n",
    "svc.fit(X_train, Y_train)\n",
    "Y_pred = svc.predict(X_test)\n",
    "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(X_train, Y_train)\n",
    "Y_pred = knn.predict(X_test)\n",
    "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "acc_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, Y_train)\n",
    "Y_pred = gaussian.predict(X_test)\n",
    "acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n",
    "acc_gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "\n",
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, Y_train)\n",
    "Y_pred = perceptron.predict(X_test)\n",
    "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n",
    "acc_perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "\n",
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(X_train, Y_train)\n",
    "Y_pred = linear_svc.predict(X_test)\n",
    "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n",
    "acc_linear_svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, Y_train)\n",
    "Y_pred = sgd.predict(X_test)\n",
    "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n",
    "acc_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(X_train, Y_train)\n",
    "Y_pred = decision_tree.predict(X_test)\n",
    "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "acc_decision_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "Y_pred = random_forest.predict(X_test)\n",
    "random_forest.score(X_train, Y_train)\n",
    "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "acc_random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models evaluation\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent', 'Linear SVC', \n",
    "              'Decision Tree'],\n",
    "    'Score': [acc_svc, acc_knn, acc_log, \n",
    "              acc_random_forest, acc_gaussian, acc_perceptron, \n",
    "              acc_sgd, acc_linear_svc, acc_decision_tree]})\n",
    "models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": data_test[\"PassengerId\"],\n",
    "        \"Survived\": Y_pred\n",
    "    })\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A better way...\n",
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #for split the data\n",
    "from sklearn.metrics import accuracy_score  #for accuracy_score\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "all_features = data_train.drop(\"Survived\",axis=1)\n",
    "Targeted_feature = data_train[\"Survived\"]\n",
    "X_train,X_test,y_train,y_test = train_test_split(all_features,Targeted_feature,test_size=0.3,random_state=42)\n",
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression # Logistic Regression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train,y_train)\n",
    "prediction_lr=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the Logistic Regression is',round(accuracy_score(prediction_lr,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_lr=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Logistic REgression is:',round(result_lr.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(criterion='gini', n_estimators=700,\n",
    "                             min_samples_split=10,min_samples_leaf=1,\n",
    "                             max_features='auto',oob_score=True,\n",
    "                             random_state=1,n_jobs=-1)\n",
    "model.fit(X_train,y_train)\n",
    "prediction_rm=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the Random Forest Classifier is',round(accuracy_score(prediction_rm,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_rm=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Random Forest Classifier is:',round(result_rm.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train,y_train)\n",
    "prediction_svm=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the Support Vector Machines Classifier is',round(accuracy_score(prediction_svm,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_svm=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Support Vector Machines Classifier is:',round(result_svm.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##knn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors = 4)\n",
    "model.fit(X_train,y_train)\n",
    "prediction_knn=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the K Nearst Neighbors Classifier is',round(accuracy_score(prediction_knn,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_knn=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for K Nearest Neighbors Classifier is:',round(result_knn.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model= GaussianNB()\n",
    "model.fit(X_train,y_train)\n",
    "prediction_gnb=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the Gaussian Naive Bayes Classifier is',round(accuracy_score(prediction_gnb,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_gnb=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Gaussian Naive Bayes classifier is:',round(result_gnb.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model= DecisionTreeClassifier(criterion='gini', \n",
    "                             min_samples_split=10,min_samples_leaf=1,\n",
    "                             max_features='auto')\n",
    "model.fit(X_train,y_train)\n",
    "prediction_tree=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the DecisionTree Classifier is',round(accuracy_score(prediction_tree,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_tree=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for Decision Tree classifier is:',round(result_tree.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model= AdaBoostClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "prediction_adb=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the AdaBoostClassifier is',round(accuracy_score(prediction_adb,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_adb=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for AdaBoostClassifier is:',round(result_adb.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "model= LinearDiscriminantAnalysis()\n",
    "model.fit(X_train,y_train)\n",
    "prediction_lda=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the LinearDiscriminantAnalysis is',round(accuracy_score(prediction_lda,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_lda=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for AdaBoostClassifier is:',round(result_lda.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model= GradientBoostingClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "prediction_gbc=model.predict(X_test)\n",
    "print('--------------The Accuracy of the model----------------------------')\n",
    "print('The accuracy of the Gradient Boosting Classifier is',round(accuracy_score(prediction_gbc,y_test)*100,2))\n",
    "kfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\n",
    "result_gbc=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\n",
    "print('The cross validated score for AdaBoostClassifier is:',round(result_gbc.mean()*100,2))\n",
    "y_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\n",
    "sns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "plt.title('Confusion_matrix', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models evaluation\n",
    "models = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'AdaBoostClassifier', \n",
    "              'Gradient Decent', 'Linear Discriminant Analysis', \n",
    "              'Decision Tree'],\n",
    "    'Score': [result_svm.mean(), result_knn.mean(), result_lr.mean(), \n",
    "              result_rm.mean(), result_gnb.mean(), result_adb.mean(), \n",
    "              result_gbc.mean(), result_lda.mean(), result_tree.mean()]})\n",
    "models.sort_values(by='Score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Hyper-Parameters Tuning in the better models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #for split the data\n",
    "from sklearn.metrics import accuracy_score  #for accuracy_score\n",
    "from sklearn.model_selection import KFold #for K-fold cross validation\n",
    "from sklearn.model_selection import cross_val_score #score evaluation\n",
    "from sklearn.model_selection import cross_val_predict #prediction\n",
    "from sklearn.metrics import confusion_matrix #for confusion matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "train_X = data_train.drop(['Survived', 'PassengerId'], axis=1)\n",
    "train_Y= data_train[\"Survived\"]\n",
    "test_X  = data_test.drop(\"PassengerId\", axis=1).copy()\n",
    "train_X.shape, train_Y.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier Parameters tunning \n",
    "model = RandomForestClassifier()\n",
    "n_estim=range(100,1000,100)\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "param_grid = {\"n_estimators\" :n_estim}\n",
    "\n",
    "\n",
    "model_rf = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "model_rf.fit(train_X,train_Y)\n",
    "\n",
    "\n",
    "\n",
    "# Best score\n",
    "print(model_rf.best_score_)\n",
    "\n",
    "#best estimator\n",
    "model_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "model= SVC()\n",
    "param_grid = {'kernel': ['rbf','linear'], \n",
    "                  'gamma': [ 0.001, 0.01, 0.1, 1],\n",
    "                  'C': [1, 10, 50, 100,200,300, 1000]}\n",
    "\n",
    "modelsvm = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "modelsvm.fit(train_X,train_Y)\n",
    "\n",
    "print(modelsvm.best_estimator_)\n",
    "\n",
    "# Best score\n",
    "print(modelsvm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing Random Forests \n",
    "# temporary...\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "random_forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=None,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "random_forest.fit(train_X, train_Y)\n",
    "Y_pred_rf = random_forest.predict(test_X)\n",
    "random_forest.score(train_X,train_Y)\n",
    "acc_random_forest = round(random_forest.score(train_X, train_Y) * 100, 2)\n",
    "\n",
    "print(\"Important features\")\n",
    "pd.Series(random_forest.feature_importances_,train_X.columns).sort_values(ascending=True).plot.barh(width=0.8)\n",
    "print('__'*30)\n",
    "print(acc_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": data_test[\"PassengerId\"],\n",
    "        \"Survived\": Y_pred_rf\n",
    "    })\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
